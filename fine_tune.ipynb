{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "In this file we will fine-tune Encoder models, like BERET, RoBERTa, etc., on the corpus of PAN. Then, evaluate them on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torchmetrics\n",
    "import wandb\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from utilities import (read_paragraphs,\n",
    "                       read_ground_truth, \n",
    "                       generate_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 更新路径 Path prefixes\n",
    "train_directory = './data/train_processed'\n",
    "train_label_directory = './data/train_label'\n",
    "\n",
    "# Due to the lack of the true test set. We use the validation set as our test set.\n",
    "# We will split the training set into train and validation sets.\n",
    "test_directory = './data/validation_processed'\n",
    "test_label_directory = './data/validation_label'\n",
    "\n",
    "checkpoint = 'bert-base-cased' #### 改这里\n",
    "run_name = 'multi_author_analyse_' + checkpoint\n",
    "# 读取段落数据\n",
    "# Read documents\n",
    "# max(end_id) = 4200\n",
    "train_data = read_paragraphs(train_directory, start_id=1, end_id=4200) # {'problem-x': [sen 1, sen 2, ...], ...}\n",
    "# max(end_id) = 900\n",
    "test_data = read_paragraphs(test_directory, start_id=1, end_id=900)\n",
    "# 读取 ground truth 数据\n",
    "# Read ground truth labels\n",
    "train_labels = read_ground_truth(train_label_directory, start_id=1, end_id=4200) # {'problem-x': [1, ...], ...}\n",
    "test_labels  = read_ground_truth(test_label_directory, start_id=1, end_id=900)\n",
    "\n",
    "# for doc_id, paragraphs in train_data.items():\n",
    "#     print(f\"{doc_id}: {paragraphs}\")\n",
    "#     print(train_labels[doc_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(train_data, train_labels, tokenizer)\n",
    "test_dataset = generate_dataset(test_data, test_labels, tokenizer)\n",
    "\n",
    "training_sets = train_dataset.train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "training_sets[\"validation\"] = training_sets.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "training_sets[\"test\"] = test_dataset\n",
    "\n",
    "training_sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = training_sets['train']['idx'][0]\n",
    "# for sen in train_data[doc]:\n",
    "#     print(sen)\n",
    "# print(train_labels[doc])\n",
    "# print('\\n')\n",
    "\n",
    "# print(training_sets['train']['sentence1'][0])\n",
    "# print(training_sets['train']['sentence2'][0])\n",
    "# print(training_sets['train']['label'][0])\n",
    "# print(training_sets['train']['idx'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], \n",
    "                     truncation=True)\n",
    "\n",
    "tokenized_datasets = training_sets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"Multi_author\") \n",
    "# On the full dataset: with batch size as 24 and gradient_accumulation_steps=4, \n",
    "# there will be 546 training steps and 182 validation steps\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"finetuned-{checkpoint}\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=run_name,  # name of the W&B run (optional)\n",
    "    logging_steps=2,  # how often to log to W&B\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_metrics = {\"F1\": torchmetrics.classification.F1Score(task='binary', num_classes=2, average=\"macro\"), \n",
    "            'Accuracy': torchmetrics.classification.BinaryAccuracy()}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    labels = torch.from_numpy(labels)\n",
    "    predictions = torch.from_numpy(np.argmax(logits, axis=-1))\n",
    "    eval_result = {}\n",
    "    for key, me in my_metrics.items():\n",
    "        eval_result[key] = me(predictions, labels).item()\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_datasets['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
