Nope, most of the west is not a desert. Most of it is grassland plains, and has been so for thousands of years. Those grassland plains are also a major breadbasket for not only our nation, but for many others as well. Now the southwest has many areas that are deserts, but there are some health benefits for people to live there. Like people who have asthma or bad allergies to plant pollen have it easier there.